{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Level Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load pre-processing data\n",
    "    - downcast to float32, int32 \n",
    "2. Train the first level models\n",
    "    - validate model with simple hold out method\n",
    "    - several models\n",
    "        - linear \n",
    "        - tree based\n",
    "        - knn\n",
    "        - kmean\n",
    "    - features gen by stacking \n",
    "3. Text features extraction \n",
    "    - tfidf + svd\n",
    "    - tfidf(Binary) + svd\n",
    "    - hash + svd\n",
    "    - hash(Binary) + svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/X_cv', '/X_test', '/X_train', '/y_cv', '/y_train']\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore('../data/feat/data.h5') as store:\n",
    "    print(store.keys())\n",
    "    X_train = store['X_train']\n",
    "    X_cv = store['X_cv']\n",
    "    y_train = store['y_train']\n",
    "    y_cv = store['y_cv']\n",
    "    X_test = store['X_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.date_block_num.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - clip to (0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.clip(0,20)\n",
    "y_cv = y_cv.clip(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10675632, 54)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- downcast to `float32`, `int32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def downcast_dtypes(df):\n",
    "#     '''\n",
    "#         Changes column types in the dataframe: \n",
    "                \n",
    "#                 `float64` type to `float32`\n",
    "#                 `int64`   type to `int32`\n",
    "#     '''\n",
    "    \n",
    "#     # Select columns to downcast\n",
    "#     float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "#     int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "#     # Downcast\n",
    "#     df[float_cols] = df[float_cols].astype(np.float32)\n",
    "#     df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = downcast_dtypes(X_train)\n",
    "# X_cv = downcast_dtypes(X_cv)\n",
    "# X_test = downcast_dtypes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = y_train.astype(np.float32)\n",
    "# y_cv = y_cv.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `fillna` with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(0,inplace=True)\n",
    "X_cv.fillna(0,inplace=True)\n",
    "X_test.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Train /test reduce\n",
    "We take only `date_block_num` between `12~32`\n",
    "- memory issue\n",
    "- time cost issue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dates = X_train.date_block_num\n",
    "\n",
    "mask = train_dates >= 12 # mask=0 : all consider , mask>=12\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "train_dates = train_dates[mask]\n",
    "test_dates = X_test.date_block_num\n",
    "cv_dates = X_cv.date_block_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6186922, 54)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. First Level Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "level2_date_block @ 31,32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dates = X_train.date_block_num\n",
    "\n",
    "# level2_date_block = [27, 28, 29, 30, 31, 32]\n",
    "level2_date_block = [31,32]\n",
    "# level2_date_block = [32]\n",
    "level2_mask = train_dates.isin(level2_date_block)\n",
    "train_dates_level2 = train_dates[level2_mask]\n",
    "train_y_level2 = y_train[level2_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(433191,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dates_level2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 custom grid search \n",
    "    - stolen from top20 kaggler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_grid_search_cv_evaluate(clf, X_train, y_train, param_grid):\n",
    "    \n",
    "    params_list = list(ParameterGrid(param_grid))\n",
    "    res_df = pd.DataFrame(data=np.zeros((len(params_list),3)), columns=['params', 'mean_test_score', 'std_test_score'])\n",
    "    res_df.loc[:,'params'] = params_list\n",
    "    \n",
    "    for i, params in enumerate(params_list):\n",
    "        scores = []\n",
    "        print('Fitting: ', params, '...', end='\\n\\tscores= ')\n",
    "        for cur_block in level2_date_block:\n",
    "            copy_clf = copy.deepcopy(clf)\n",
    "            original_param = copy_clf.get_params()\n",
    "            original_param.update(params)\n",
    "            copy_clf.set_params(**original_param) # update copy clf with trying params \n",
    "            \n",
    "            copy_clf.fit(X_train[train_dates < cur_block].values, y_train[train_dates < cur_block])\n",
    "            pred_y = copy_clf.predict(X_train[train_dates == cur_block].values)\n",
    "            pred_y = np.clip(pred_y, 0., 20.)\n",
    "            score = mean_squared_error(y_train[train_dates == cur_block], pred_y)**.5\n",
    "            print('{:.5f} '.format(score), end='')\n",
    "            scores.append(score)\n",
    "            del copy_clf; gc.collect()\n",
    "        \n",
    "        print('')\n",
    "        res_df.loc[i, 'mean_test_score'] = np.mean(scores)\n",
    "        res_df.loc[i, 'std_test_score'] = np.std(scores)\n",
    "        \n",
    "    print('Fitting finished')\n",
    "    res_df = res_df.sort_values(by=['mean_test_score', 'std_test_score'], ascending=True).reset_index(drop=True)\n",
    "    best_params = res_df.loc[0, 'params']\n",
    "    \n",
    "    print('Selected hyper-params:', best_params)\n",
    "    print('cv score: {:.4f}, std: {:.4f}'.format(res_df.loc[0, 'mean_test_score'], res_df.loc[0, 'std_test_score']))\n",
    "    del res_df, params_list; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_grid_search_simple_holdout_evaluate(clf, X_train, y_train, param_grid, level2_date_block=[32]):\n",
    "    \n",
    "    params_list = list(ParameterGrid(param_grid))\n",
    "    res_df = pd.DataFrame(data=np.zeros((len(params_list),2)), columns=['params', 'val_score'])\n",
    "    res_df.loc[:,'params'] = params_list\n",
    "    \n",
    "    train_mask = train_dates < level2_date_block[0]\n",
    "    validation_mask = (train_dates >= level2_date_block[0]) & (train_dates <= level2_date_block[-1])\n",
    "    for i, params in enumerate(params_list):\n",
    "        print('Fitting: ', params, '...', end='\\n\\tscore= ')\n",
    "        \n",
    "        copy_clf = copy.deepcopy(clf)\n",
    "        original_param = copy_clf.get_params()\n",
    "        original_param.update(params)\n",
    "        copy_clf.set_params(**original_param) # update copy clf with trying params \n",
    "\n",
    "        copy_clf.fit(X_train[train_mask].values, y_train[train_mask])\n",
    "        pred_y = copy_clf.predict(X_train[validation_mask].values)\n",
    "        pred_y = np.clip(pred_y, 0., 20.)\n",
    "        score = mean_squared_error(y_train[validation_mask], pred_y)**.5\n",
    "        print('{:.5f} '.format(score), end='\\n')\n",
    "        del copy_clf; gc.collect()\n",
    "\n",
    "        res_df.loc[i, 'val_score'] = score\n",
    "        \n",
    "    print('Fitting finished')\n",
    "    res_df = res_df.sort_values(by=['val_score'], ascending=True).reset_index(drop=True)\n",
    "    best_params = res_df.loc[0, 'params']\n",
    "    \n",
    "    print('Selected hyper-params:', best_params)\n",
    "    print('cv score: {:.4f}'.format(res_df.loc[0, 'val_score']))\n",
    "    del res_df, params_list; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Optimization\n",
    "- with `hyperopt` library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- simple hold out method to optimize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    '''\n",
    "        calculate rmse with simple holdout method \n",
    "        \n",
    "        -- train on `train_dates < level2_date_block[0]` \n",
    "        -- validate on `leve2_date_block`  (date_block_num : 27-32)\n",
    "    '''\n",
    "    ## extract clf and new_params from `params`\n",
    "    clf = params.get('clf')\n",
    "    new_params = params.get('params') ## update params\n",
    "    \n",
    "    ## update params\n",
    "    copy_clf = copy.deepcopy(clf)\n",
    "    original_params = copy_clf.get_params()\n",
    "    original_params.update(new_params)\n",
    "    copy_clf.set_params(**original_params) # update copy clf with trying new_params     \n",
    "    \n",
    "    train_mask = train_dates < level2_date_block[0]\n",
    "    \n",
    "    ## simple hold out method\n",
    "    ## validate on the level2_date_block\n",
    "    validation_mask = (train_dates >= level2_date_block[0]) & (train_dates <= level2_date_block[-1]) \n",
    "    \n",
    "    copy_clf.fit(X_train[train_mask].values, y_train[train_mask]) ## train on <27 , validate on 27-32\n",
    "        \n",
    "    pred_y = copy_clf.predict(X_train[validation_mask].values)\n",
    "    pred_y = np.clip(pred_y, 0., 20.) # clip to (0,20)\n",
    "    rmse = mean_squared_error(y_train[validation_mask], pred_y)**.5\n",
    "    print('rmse: {:.5f} '.format(rmse), end='\\t')\n",
    "    print('params: {}'.format(new_params))\n",
    "    \n",
    "    return {'loss':rmse,'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model \n",
    "- Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso #featuring L2/L1 regularized linear models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting:  {'rg__alpha': 0.0} ...\n",
      "\tscore= 0.96775 \n",
      "Fitting:  {'rg__alpha': 0.20000000000000001} ...\n",
      "\tscore= 0.96510 \n",
      "Fitting:  {'rg__alpha': 0.40000000000000002} ...\n",
      "\tscore= 0.96600 \n",
      "Fitting:  {'rg__alpha': 0.60000000000000009} ...\n",
      "\tscore= 0.96738 \n",
      "Fitting:  {'rg__alpha': 0.80000000000000004} ...\n",
      "\tscore= 0.96910 \n",
      "Fitting finished\n",
      "Selected hyper-params: {'rg__alpha': 0.20000000000000001}\n",
      "cv score: 0.9651\n"
     ]
    }
   ],
   "source": [
    "## try simple-hold-out grid serach\n",
    "rg_clf = Pipeline([\n",
    "    ('standardscaler', StandardScaler(copy=False, with_mean=True, with_std=True)),\n",
    "    ('rg', Ridge(fit_intercept=True, normalize=True, max_iter=2000, random_state=0))\n",
    "])\n",
    "rg_params = {\n",
    "    'rg__alpha': np.arange(0,1,0.2)\n",
    "}\n",
    "\n",
    "customized_grid_search_simple_holdout_evaluate(rg_clf,X_train,y_train,param_grid=rg_params,level2_date_block=level2_date_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 1.00357 \tparams: {'rg__alpha': 0.04080360347886225}\n",
      "rmse: 1.00317 \tparams: {'rg__alpha': 0.025410732521240376}\n",
      "rmse: 1.00342 \tparams: {'rg__alpha': 0.034911349319165064}\n",
      "rmse: 1.01571 \tparams: {'rg__alpha': 0.7118020171649374}\n",
      "rmse: 1.01847 \tparams: {'rg__alpha': 0.9214224958287398}\n",
      "rmse: 1.00790 \tparams: {'rg__alpha': 0.23520375805316973}\n",
      "rmse: 1.01593 \tparams: {'rg__alpha': 0.7279100564363649}\n",
      "rmse: 1.01644 \tparams: {'rg__alpha': 0.7650441014491024}\n",
      "rmse: 1.01877 \tparams: {'rg__alpha': 0.945986312934316}\n",
      "rmse: 1.01007 \tparams: {'rg__alpha': 0.35088290894136054}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "rg_clf = Pipeline([\n",
    "    ('standardscaler', StandardScaler(copy=False, with_mean=True, with_std=True)),\n",
    "    ('rg', Ridge(fit_intercept=True, normalize=True, max_iter=2000, random_state=0))\n",
    "])\n",
    "rg_params = {\n",
    "    'clf':rg_clf,\n",
    "    'params': {'rg__alpha': hp.uniform('rg__alpha', 0, 1)},\n",
    "}\n",
    "\n",
    "best = fmin(fn = objective,\n",
    "            space = rg_params,\n",
    "            algo = tpe.suggest,\n",
    "            trials = trials,\n",
    "            max_evals = 10\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">rmse:1.00317\n",
    " - alpha : 0.02541"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* linear svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "     random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.LinearSVR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 1.00416 \tparams: {'lasso__alpha': 0.009869086395721482}\n",
      "rmse: 1.02777 \tparams: {'lasso__alpha': 0.0907611500382878}\n",
      "rmse: 1.02902 \tparams: {'lasso__alpha': 0.09572609233971596}\n",
      "rmse: 1.02897 \tparams: {'lasso__alpha': 0.09552780779733748}\n",
      "rmse: 1.01632 \tparams: {'lasso__alpha': 0.050759576343417726}\n",
      "rmse: 1.01541 \tparams: {'lasso__alpha': 0.047752693076762553}\n",
      "rmse: 1.01148 \tparams: {'lasso__alpha': 0.03489274382258616}\n",
      "rmse: 1.00366 \tparams: {'lasso__alpha': 0.006940518212747282}\n",
      "rmse: 1.01811 \tparams: {'lasso__alpha': 0.05646730195223748}\n",
      "rmse: 1.02094 \tparams: {'lasso__alpha': 0.06545538392838314}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "lasso_clf = Pipeline([\n",
    "    ('standardscaler', StandardScaler(copy=False, with_mean=True, with_std=True)),\n",
    "    ('lasso', Lasso(random_state=0))\n",
    "])\n",
    "\n",
    "lasso_params = {\n",
    "    'clf':lasso_clf,\n",
    "    'params': {\n",
    "        'lasso__alpha': hp.uniform('lasso__alpha',0,0.1)\n",
    "    },    \n",
    "}\n",
    "\n",
    "best = fmin(fn = objective,\n",
    "            space = lasso_params,\n",
    "            algo = tpe.suggest,\n",
    "            trials = trials,\n",
    "            max_evals = 10\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.96565 \tparams: {'lasso__alpha': 0.021311}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.9656547294753367, 'status': 'ok'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lasso_params = {\n",
    "#     'clf':lasso_clf,\n",
    "#     'params': {\n",
    "#         'lasso__alpha': 0.021311\n",
    "#     },    \n",
    "# }\n",
    "# objective(lasso_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> rmse: 1.00366\n",
    "    - alpha: 0.00694"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree based model \n",
    "- lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
       "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
       "       n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
       "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMRegressor(random_state=0, n_jobs=4)\n",
    "\n",
    "# 1. find an optimal n_esti for rather larger learning rate\n",
    "lgb_params = {\n",
    "    'boosting_type': ['gbdt', 'dart']\n",
    "}\n",
    "\n",
    "\n",
    "customized_grid_search_simple_holdout_evaluate(lgb_clf, X_train, y_train, lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting:  {'learning_rate': 0.2, 'n_estimators': 10} ...\n",
      "\tscore= 0.90978 \n",
      "Fitting:  {'learning_rate': 0.2, 'n_estimators': 20} ...\n",
      "\tscore= 0.89607 \n",
      "Fitting:  {'learning_rate': 0.2, 'n_estimators': 40} ...\n",
      "\tscore= 0.89238 \n",
      "Fitting:  {'learning_rate': 0.2, 'n_estimators': 60} ...\n",
      "\tscore= 0.89089 \n",
      "Fitting:  {'learning_rate': 0.2, 'n_estimators': 80} ...\n",
      "\tscore= 0.89274 \n",
      "Fitting:  {'learning_rate': 0.1, 'n_estimators': 10} ...\n",
      "\tscore= 0.94879 \n",
      "Fitting:  {'learning_rate': 0.1, 'n_estimators': 20} ...\n",
      "\tscore= 0.91117 \n",
      "Fitting:  {'learning_rate': 0.1, 'n_estimators': 40} ...\n",
      "\tscore= 0.89477 \n",
      "Fitting:  {'learning_rate': 0.1, 'n_estimators': 60} ...\n",
      "\tscore= 0.89334 \n",
      "Fitting:  {'learning_rate': 0.1, 'n_estimators': 80} ...\n",
      "\tscore= 0.89265 \n",
      "Fitting:  {'learning_rate': 0.075, 'n_estimators': 10} ...\n",
      "\tscore= 0.97447 \n",
      "Fitting:  {'learning_rate': 0.075, 'n_estimators': 20} ...\n",
      "\tscore= 0.92518 \n",
      "Fitting:  {'learning_rate': 0.075, 'n_estimators': 40} ...\n",
      "\tscore= 0.90328 \n",
      "Fitting:  {'learning_rate': 0.075, 'n_estimators': 60} ...\n",
      "\tscore= "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-34b056cdb550>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.075\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m }\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcustomized_grid_search_simple_holdout_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlgb_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlgb_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-7fc7631bf6c2>\u001b[0m in \u001b[0;36mcustomized_grid_search_simple_holdout_evaluate\u001b[1;34m(clf, X_train, y_train, param_grid, level2_date_block)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mcopy_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moriginal_param\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# update copy clf with trying params\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mcopy_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mpred_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalidation_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mpred_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\py36\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    616\u001b[0m                                        \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                                        \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m                                        callbacks=callbacks)\n\u001b[0m\u001b[0;32m    619\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\py36\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    471\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\py36\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    199\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\py36\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1519\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1521\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1523\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lgb_clf = lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.1, random_state=0, n_jobs =4)\n",
    "\n",
    "# 1. find an optimal n_esti for rather larger learning rate\n",
    "lgb_params = {\n",
    "    'n_estimators': [10, 20, 40, 60, 80],\n",
    "    'learning_rate': [0.2, 0.1, 0.075, 0.05]\n",
    "}\n",
    "customized_grid_search_simple_holdout_evaluate(lgb_clf, X_train, y_train, lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.1, n_estimators=40,\n",
    "                            random_state=0, n_jobs =4)\n",
    "\n",
    "\n",
    "\n",
    "# 2. tune tree-specific params\n",
    "lgb_params = {\n",
    "    'max_depth': [4, 6, 8, 10, 12],\n",
    "}\n",
    "customized_grid_search_simple_holdout_evaluate(lgb_clf, X_train, y_train, lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.1, n_estimators=40,\n",
    "                            max_depth=12, \n",
    "                            random_state=0, n_jobs =4)\n",
    "\n",
    "\n",
    "# 2. tune tree-specific params\n",
    "lgb_params = {\n",
    "    'min_split_gain': [.0, .1, .2],\n",
    "    'min_child_samples': [20, 40, 80], ## min_data_in_leaf\n",
    "}\n",
    "customized_grid_search_simple_holdout_evaluate(lgb_clf, X_train, y_train, lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.1, n_estimators=40,\n",
    "                            max_depth=12, min_child_samples=20, min_split_gain=0.0,\n",
    "                            random_state=0, n_jobs=4)\n",
    "\n",
    "# 2. tune tree-specific params\n",
    "lgb_params = {\n",
    "    'subsample': [.2, .4, .6, .8, 1.],\n",
    "    'colsample_bytree': [.2, .4, .6, .8, 1.]\n",
    "}\n",
    "customized_grid_search_simple_holdout_evaluate(lgb_clf, X_train, y_train, lgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> use hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.86781 \tparams: {'bagging_fraction': 0.8526347938264912, 'boosting_type': 'dart', 'feature_fraction': 0.3713832005356714, 'gamma': 0.3513437876057838, 'max_depth': 12, 'num_leaves': 36, 'reg_lambda': 0.3314183540297525}\n",
      "rmse: 0.88221 \tparams: {'bagging_fraction': 0.7561365505609431, 'boosting_type': 'dart', 'feature_fraction': 0.6631183939697767, 'gamma': 0.10476140323208019, 'max_depth': 4, 'num_leaves': 30, 'reg_lambda': 0.6313148071988549}\n",
      "rmse: 0.83194 \tparams: {'bagging_fraction': 0.9187387422237163, 'boosting_type': 'gbdt', 'feature_fraction': 0.8084801260455161, 'gamma': 0.25342233715729756, 'max_depth': 12, 'num_leaves': 36, 'reg_lambda': 0.9346223808836792}\n",
      "rmse: 0.83162 \tparams: {'bagging_fraction': 0.8914728193944422, 'boosting_type': 'gbdt', 'feature_fraction': 0.6805724910142892, 'gamma': 0.49912184111068736, 'max_depth': 7, 'num_leaves': 106, 'reg_lambda': 0.4603614731830482}\n",
      "rmse: 0.85809 \tparams: {'bagging_fraction': 0.8772487776479582, 'boosting_type': 'dart', 'feature_fraction': 0.4096305424714555, 'gamma': 0.34929281868219775, 'max_depth': 13, 'num_leaves': 80, 'reg_lambda': 0.37188951741776977}\n",
      "rmse: 0.86709 \tparams: {'bagging_fraction': 0.7016913073260398, 'boosting_type': 'dart', 'feature_fraction': 0.8560400574024338, 'gamma': 0.4246273160859152, 'max_depth': 13, 'num_leaves': 16, 'reg_lambda': 0.09779791198380083}\n",
      "rmse: 0.88383 \tparams: {'bagging_fraction': 0.8273198270382809, 'boosting_type': 'dart', 'feature_fraction': 0.40781683054464307, 'gamma': 0.1742814800689673, 'max_depth': 13, 'num_leaves': 10, 'reg_lambda': 0.49649820520767607}\n",
      "rmse: 0.87527 \tparams: {'bagging_fraction': 0.9535359532202721, 'boosting_type': 'dart', 'feature_fraction': 0.6018465918295672, 'gamma': 0.1382373751404884, 'max_depth': 6, 'num_leaves': 20, 'reg_lambda': 0.2261521732335654}\n",
      "rmse: 0.88467 \tparams: {'bagging_fraction': 0.9404125524291072, 'boosting_type': 'dart', 'feature_fraction': 0.32254933830736887, 'gamma': 0.2679727801814501, 'max_depth': 5, 'num_leaves': 62, 'reg_lambda': 0.0223068806459501}\n",
      "rmse: 0.86143 \tparams: {'bagging_fraction': 0.8397376580620999, 'boosting_type': 'dart', 'feature_fraction': 0.43487846011614495, 'gamma': 0.1389156916188244, 'max_depth': 15, 'num_leaves': 48, 'reg_lambda': 0.059362258036086635}\n"
     ]
    }
   ],
   "source": [
    "lgb_clf = lgb.LGBMRegressor(random_state=0, n_jobs=8)\n",
    "trials = Trials()\n",
    "# 1. find an optimal n_esti for rather larger learning rate\n",
    "lgb_params = {\n",
    "    'clf' : lgb_clf,\n",
    "    'params': {\n",
    "        'boosting_type': hp.choice('boosting_type',['gbdt', 'dart']), ## gbdt \n",
    "#         'boosting_type': []'gbdt',\n",
    "        'num_leaves'   : hp.choice('num_leaves', np.arange(8,129,2,dtype=int)),\n",
    "        'max_depth' : hp.choice(\"max_depth\", np.arange(4, 17, dtype=int)),    \n",
    "        'feature_fraction': hp.uniform('feature_fraction', 0.3, 1.0),\n",
    "        'bagging_fraction': hp.uniform ('bagging_fraction', 0.7, 1),\n",
    "        'reg_lambda': hp.uniform('reg_lambda',0,1),\n",
    "        'gamma' : hp.uniform('gamma', 0.1,0.5)\n",
    "    }\n",
    "    \n",
    "}\n",
    "best =fmin(fn = objective,\n",
    "           space = lgb_params,\n",
    "           algo = tpe.suggest,\n",
    "           trials = trials,\n",
    "           max_evals = 10\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.8316196011425457, 'status': 'ok'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.best_trial['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "rmse: 0.83162 \t\n",
    "\n",
    "params: {'bagging_fraction': 0.8914728193944422, 'boosting_type': 'gbdt', 'feature_fraction': 0.6805724910142892, 'gamma': 0.49912184111068736, 'max_depth': 7, 'num_leaves': 106, 'reg_lambda': 0.4603614731830482}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.81909 \tparams: {'bagging_fraction': 0.9568845079308161, 'bossting_type': 'gbdt', 'feature_fraction': 0.6203248801718259, 'gamma': 0.39624896070423066, 'max_depth': 12, 'metric': 'rmse', 'num_leaves': 64, 'objective': 'regression', 'reg_lambda': 0.38856229720270463}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.81908550940125358, 'status': 'ok'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## use pre-train params from lgb_model1\n",
    "# lgb_clf = lgb.LGBMRegressor(random_state=0)\n",
    "# lgb_params = {\n",
    "#     'clf':lgb_clf,\n",
    "#     'params':{\n",
    "#         'bagging_fraction': 0.9568845079308161,\n",
    "#         'bossting_type': 'gbdt',\n",
    "#         'feature_fraction': 0.6203248801718259,\n",
    "#         'gamma': 0.39624896070423066,\n",
    "#         'max_depth': 12,\n",
    "#         'metric': 'rmse',\n",
    "#         'num_leaves': 64,\n",
    "#         'objective': 'regression',\n",
    "#         'reg_lambda': 0.38856229720270463\n",
    "#     }\n",
    "# }\n",
    "# objective(lgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> rmse: 0.81909\n",
    "    - {'bagging_fraction': 0.9568845079308161, 'bossting_type': 'gbdt', 'feature_fraction': 0.6203248801718259, 'gamma': 0.39624896070423066, 'max_depth': 12, 'metric': 'rmse', 'num_leaves': 64, 'objective': 'regression', 'reg_lambda': 0.38856229720270463}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.85051 \tparams: {'max_depth': 16, 'min_samples_leaf': 30, 'min_samples_split': 400, 'n_estimators': 150}\n",
      "rmse: 0.88084 \tparams: {'max_depth': 8, 'min_samples_leaf': 30, 'min_samples_split': 200, 'n_estimators': 300}\n",
      "rmse: 0.92693 \tparams: {'max_depth': 4, 'min_samples_leaf': 30, 'min_samples_split': 400, 'n_estimators': 250}\n",
      "rmse: 0.86244 \tparams: {'max_depth': 12, 'min_samples_leaf': 50, 'min_samples_split': 400, 'n_estimators': 200}\n",
      "rmse: 0.92699 \tparams: {'max_depth': 4, 'min_samples_leaf': 80, 'min_samples_split': 200, 'n_estimators': 250}\n",
      "rmse: 0.84882 \tparams: {'max_depth': 16, 'min_samples_leaf': 40, 'min_samples_split': 100, 'n_estimators': 300}\n",
      "rmse: 0.85072 \tparams: {'max_depth': 16, 'min_samples_leaf': 70, 'min_samples_split': 100, 'n_estimators': 300}\n",
      "rmse: 0.92655 \tparams: {'max_depth': 4, 'min_samples_leaf': 50, 'min_samples_split': 200, 'n_estimators': 200}\n",
      "rmse: 0.92616 \tparams: {'max_depth': 4, 'min_samples_leaf': 70, 'min_samples_split': 200, 'n_estimators': 100}\n",
      "rmse: 0.86209 \tparams: {'max_depth': 12, 'min_samples_leaf': 30, 'min_samples_split': 400, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestRegressor(min_samples_split=300, min_samples_leaf=30, max_features='sqrt',n_estimators=50,\n",
    "                               max_depth=4, n_jobs=4, criterion='mse',random_state=0)\n",
    "\n",
    "rf_clf.estimators_=1 # need to set a value otherwise rise AttributeError in hyperopt\n",
    "rf_params = {\n",
    "    'clf' : rf_clf,\n",
    "    'params' : {\n",
    "        'min_samples_split' : hp.choice('min_samples_split',np.arange(100,500,100)),\n",
    "        'min_samples_leaf': hp.choice('min_samples_leaf',np.arange(30,100,10)),\n",
    "        'n_estimators' : hp.choice('n_estimators', np.arange(50,301,50)),\n",
    "        'max_depth': hp.choice('max_depth',[4, 8, 12, 16])        \n",
    "    }    \n",
    "}\n",
    "\n",
    "best =fmin(fn = objective,\n",
    "           space = rf_params,\n",
    "           algo = tpe.suggest,\n",
    "           max_evals = 10\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> rmse: `0.84882`\n",
    " - max_depth = `16`\n",
    " - min_samples_leaf = `40`\n",
    " - min_samples_split = `100`\n",
    " - n_estimators = `300`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN  model\n",
    "\n",
    "- KNN: Prediction and Neighbor distances features\n",
    "\n",
    " Time cost a lot(>2 days), I give it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "          metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 1.01656 \tparams: {'n_neighbors': 40, 'p': 1, 'weights': 'uniform'}\n",
      "rmse: 1.01279 \tparams: {'n_neighbors': 80, 'p': 1, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "knn_clf = KNeighborsRegressor(algorithm='auto', leaf_size=100, metric='minkowski',\n",
    "                              metric_params=None, n_jobs=4, n_neighbors=5, p=1,\n",
    "                              weights='uniform')\n",
    "knn_params = {\n",
    "    'clf' : knn_clf,\n",
    "    'params' :{\n",
    "        'p': hp.choice('p',[1,2]),\n",
    "        'weights': hp.choice('weights',['uniform', 'distance']),\n",
    "        'n_neighbors': hp.choice('n_neighbors', np.arange(10, 101, 10,dtype=int))\n",
    "    }\n",
    "}\n",
    "best = fmin(fn = objective,\n",
    "            space = knn_params,\n",
    "            algo = tpe.suggest,\n",
    "            trials = trials,\n",
    "            max_evals = 5\n",
    "           )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> knn: \n",
    "    - n_neighbors = `15`\n",
    "    - weights = `distance`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans\n",
    "- mini-batch kmeans cast to low dimensional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.metrics import calinski_harabaz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238172, 54)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.concat([X_train, X_cv,X_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6639294, 54)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>item_id_avg_item_price_lag_1</th>\n",
       "      <th>item_id_sum_item_cnt_day_lag_1</th>\n",
       "      <th>item_id_avg_item_cnt_day_lag_1</th>\n",
       "      <th>shop_id_avg_item_price_lag_1</th>\n",
       "      <th>shop_id_sum_item_cnt_day_lag_1</th>\n",
       "      <th>shop_id_avg_item_cnt_day_lag_1</th>\n",
       "      <th>...</th>\n",
       "      <th>item_id_avg_item_price_lag_12</th>\n",
       "      <th>item_id_sum_item_cnt_day_lag_12</th>\n",
       "      <th>item_id_avg_item_cnt_day_lag_12</th>\n",
       "      <th>shop_id_avg_item_price_lag_12</th>\n",
       "      <th>shop_id_sum_item_cnt_day_lag_12</th>\n",
       "      <th>shop_id_avg_item_cnt_day_lag_12</th>\n",
       "      <th>item_category_id_avg_item_price_lag_12</th>\n",
       "      <th>item_category_id_sum_item_cnt_day_lag_12</th>\n",
       "      <th>item_category_id_avg_item_cnt_day_lag_12</th>\n",
       "      <th>item_cnt_month_lag_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6639289</th>\n",
       "      <td>45</td>\n",
       "      <td>18454</td>\n",
       "      <td>34</td>\n",
       "      <td>55</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1176.795898</td>\n",
       "      <td>702.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6639290</th>\n",
       "      <td>45</td>\n",
       "      <td>16188</td>\n",
       "      <td>34</td>\n",
       "      <td>64</td>\n",
       "      <td>1359.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1176.795898</td>\n",
       "      <td>702.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6639291</th>\n",
       "      <td>45</td>\n",
       "      <td>15757</td>\n",
       "      <td>34</td>\n",
       "      <td>55</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1176.795898</td>\n",
       "      <td>702.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>...</td>\n",
       "      <td>199.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1273.734375</td>\n",
       "      <td>1251.0</td>\n",
       "      <td>1.268763</td>\n",
       "      <td>297.181396</td>\n",
       "      <td>9809.0</td>\n",
       "      <td>1.041406</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6639292</th>\n",
       "      <td>45</td>\n",
       "      <td>19648</td>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "      <td>89.099998</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1176.795898</td>\n",
       "      <td>702.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6639293</th>\n",
       "      <td>45</td>\n",
       "      <td>969</td>\n",
       "      <td>34</td>\n",
       "      <td>37</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1176.795898</td>\n",
       "      <td>702.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>...</td>\n",
       "      <td>549.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1273.734375</td>\n",
       "      <td>1251.0</td>\n",
       "      <td>1.268763</td>\n",
       "      <td>457.671997</td>\n",
       "      <td>5185.0</td>\n",
       "      <td>1.076620</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         shop_id  item_id  date_block_num  item_category_id  \\\n",
       "6639289       45    18454              34                55   \n",
       "6639290       45    16188              34                64   \n",
       "6639291       45    15757              34                55   \n",
       "6639292       45    19648              34                40   \n",
       "6639293       45      969              34                37   \n",
       "\n",
       "         item_id_avg_item_price_lag_1  item_id_sum_item_cnt_day_lag_1  \\\n",
       "6639289                     99.000000                             2.0   \n",
       "6639290                   1359.000000                             1.0   \n",
       "6639291                    229.000000                             5.0   \n",
       "6639292                     89.099998                             2.0   \n",
       "6639293                    198.000000                             3.0   \n",
       "\n",
       "         item_id_avg_item_cnt_day_lag_1  shop_id_avg_item_price_lag_1  \\\n",
       "6639289                             1.0                   1176.795898   \n",
       "6639290                             1.0                   1176.795898   \n",
       "6639291                             1.0                   1176.795898   \n",
       "6639292                             1.0                   1176.795898   \n",
       "6639293                             1.0                   1176.795898   \n",
       "\n",
       "         shop_id_sum_item_cnt_day_lag_1  shop_id_avg_item_cnt_day_lag_1  \\\n",
       "6639289                           702.0                           1.125   \n",
       "6639290                           702.0                           1.125   \n",
       "6639291                           702.0                           1.125   \n",
       "6639292                           702.0                           1.125   \n",
       "6639293                           702.0                           1.125   \n",
       "\n",
       "                 ...            item_id_avg_item_price_lag_12  \\\n",
       "6639289          ...                                      0.0   \n",
       "6639290          ...                                      0.0   \n",
       "6639291          ...                                    199.0   \n",
       "6639292          ...                                      0.0   \n",
       "6639293          ...                                    549.0   \n",
       "\n",
       "         item_id_sum_item_cnt_day_lag_12  item_id_avg_item_cnt_day_lag_12  \\\n",
       "6639289                              0.0                              0.0   \n",
       "6639290                              0.0                              0.0   \n",
       "6639291                              9.0                              1.0   \n",
       "6639292                              0.0                              0.0   \n",
       "6639293                              6.0                              1.0   \n",
       "\n",
       "         shop_id_avg_item_price_lag_12  shop_id_sum_item_cnt_day_lag_12  \\\n",
       "6639289                       0.000000                              0.0   \n",
       "6639290                       0.000000                              0.0   \n",
       "6639291                    1273.734375                           1251.0   \n",
       "6639292                       0.000000                              0.0   \n",
       "6639293                    1273.734375                           1251.0   \n",
       "\n",
       "         shop_id_avg_item_cnt_day_lag_12  \\\n",
       "6639289                         0.000000   \n",
       "6639290                         0.000000   \n",
       "6639291                         1.268763   \n",
       "6639292                         0.000000   \n",
       "6639293                         1.268763   \n",
       "\n",
       "         item_category_id_avg_item_price_lag_12  \\\n",
       "6639289                                0.000000   \n",
       "6639290                                0.000000   \n",
       "6639291                              297.181396   \n",
       "6639292                                0.000000   \n",
       "6639293                              457.671997   \n",
       "\n",
       "         item_category_id_sum_item_cnt_day_lag_12  \\\n",
       "6639289                                       0.0   \n",
       "6639290                                       0.0   \n",
       "6639291                                    9809.0   \n",
       "6639292                                       0.0   \n",
       "6639293                                    5185.0   \n",
       "\n",
       "         item_category_id_avg_item_cnt_day_lag_12  item_cnt_month_lag_12  \n",
       "6639289                                  0.000000                    0.0  \n",
       "6639290                                  0.000000                    0.0  \n",
       "6639291                                  1.041406                    0.0  \n",
       "6639292                                  0.000000                    0.0  \n",
       "6639293                                  1.076620                    0.0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters = 4 score= 341409.019388\n",
      "n_clusters = 6 score= 263314.025485\n",
      "n_clusters = 8 score= 809444.498912\n",
      "n_clusters = 10 score= 723085.429986\n",
      "n_clusters = 12 score= 744213.776063\n",
      "n_clusters = 14 score= 632340.335443\n"
     ]
    }
   ],
   "source": [
    "for c in np.arange(4,16,2):\n",
    "    print('n_clusters =', c, end=' score= ')\n",
    "    km = Pipeline([\n",
    "        ('standardscaler', StandardScaler(copy=False, with_mean=True, with_std=True)),\n",
    "        ('kmean', MiniBatchKMeans(n_clusters=c, max_no_improvement=30, \n",
    "                                  verbose=0, batch_size=1000000, random_state=0))\n",
    "    ])\n",
    "#     mini_kmean = MiniBatchKMeans(n_clusters=8, batch_size=10000, verbose=2, random_state=0)\n",
    "    labels = km.fit_predict(merge)\n",
    "    print(calinski_harabaz_score(merge, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `n_cluster = 8` ... highest score means better clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dates = np.concatenate([train_dates,cv_dates, test_dates], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6639294,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_dates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6639294,)\n"
     ]
    }
   ],
   "source": [
    "merge_y = np.concatenate([y_train,y_cv, np.zeros((len(test_dates),))], axis=0)\n",
    "print(merge_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_train_dates = merge_dates[merge_dates>=level2_date_block[0]] ## 27 - 34\n",
    "stage2_train_mask = (stage2_train_dates <= level2_date_block[-1]) # train on : 27-32\n",
    "stage2_valid_mask = (stage2_train_dates == 33) ## validate on : 33\n",
    "stage2_final_train_mask = (stage2_train_dates <= 33) ## \n",
    "stage2_test_mask = (stage2_train_dates==34)\n",
    "stage2_train_y = merge_y[merge_dates>=level2_date_block[0]] ## 27-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_models = {\n",
    "    'rg': Pipeline([\n",
    "                ('standardscaler', StandardScaler(copy=False, with_mean=True, with_std=True)),\n",
    "                ('rg', Ridge(alpha=0.025, fit_intercept=True, normalize=True, max_iter=2000, random_state=0))\n",
    "            ]),\n",
    "    'lasso':Pipeline([\n",
    "                    ('standardscaler', StandardScaler(copy=False, with_mean=True, with_std=True)),\n",
    "                    ('lasso', Lasso(alpha=0.0069, normalize=False, fit_intercept=True, max_iter=2000, random_state=0))\n",
    "                ]),\n",
    "    'rf': RandomForestRegressor(n_estimators=300,\n",
    "                               min_samples_split=100, min_samples_leaf=40, max_features='sqrt',\n",
    "                               max_depth=16, n_jobs=4, criterion='mse', random_state=0),\n",
    "    'lgbm': lgb.LGBMRegressor(boosting_type='gbdt', \n",
    "                                max_depth=7,\n",
    "                                num_leaves=106,\n",
    "                                bagging_fraction=0.8914728193944422,\n",
    "                                feature_fraction=0.6805724910142892,\n",
    "                                reg_lambda = 0.4603614731830482,\n",
    "                                gamma = 0.49912184111068736,\n",
    "                                metric='rmse',\n",
    "                                n_jobs=4,\n",
    "                                random_state=0)\n",
    "}\n",
    "\n",
    " \n",
    "# knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "#                               metric_params=None, n_jobs=4, n_neighbors=15, p=1,\n",
    "#                               weights='distance')\n",
    "\n",
    "mini_kmean = Pipeline([\n",
    "                ('standardscaler', StandardScaler(copy=False, with_mean=True, with_std=True)),\n",
    "                ('kmean', MiniBatchKMeans(n_clusters=8, max_no_improvement=30, \n",
    "                                          verbose=0, batch_size=1000000, random_state=0))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(885563,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage2_train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage2_feature_generate():\n",
    "    \n",
    "#     print('Training supervised models')\n",
    "    \n",
    "    all_preds = []\n",
    "    for model_name, model in supervised_models.items():\n",
    "        \n",
    "        print(model_name, end=': ')\n",
    "        preds = []\n",
    "        for cur_block in np.arange(31,35,1):#np.arange(27, 35, 1):\n",
    "            X_tr = merge[merge_dates < cur_block].values ## ndarray\n",
    "            y_tr = merge_y[merge_dates < cur_block]\n",
    "            X_test = merge[merge_dates == cur_block].values\n",
    "            \n",
    "            copy_clf = copy.deepcopy(model)\n",
    "            copy_clf.fit(X_tr, y_tr)\n",
    "            pred_test = copy_clf.predict(X_test)\n",
    "            pred_test = np.clip(pred_test, 0., 20.)\n",
    "            preds.append(pred_test)\n",
    "            print(cur_block, end=' ')\n",
    "\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        preds = preds.reshape((len(preds), 1))\n",
    "        all_preds.append(preds)\n",
    "    \n",
    "        print('')\n",
    "        \n",
    "    #knn:\n",
    "#     print('knn')\n",
    "#     X_tr = merge[merge_dates < level2_date_block[0]].values\n",
    "#     y_tr = merge_y[merge_dates < level2_date_block[0]]\n",
    "\n",
    "#     X_test = merge[merge_dates >= level2_date_block[0]].values\n",
    "#     knn.fit(X_tr, y_tr)\n",
    "#     knn_pred = knn.predict(X_test)\n",
    "#     knn_pred = np.clip(knn_pred, 0., 20.).reshape((len(knn_pred), 1))\n",
    "#     knn_dist = knn.kneighbors(X_test, return_distance=True)[0] # distances\n",
    "#     print(np.array(knn_dist).shape)\n",
    "#     all_preds.append(knn_pred)\n",
    "#     all_preds.append(knn_dist)\n",
    "    \n",
    "    # kmeans\n",
    "    X_test = merge[merge_dates >= level2_date_block[0]].values # >=27\n",
    "    mini_kmean.fit(merge.values)\n",
    "    kmean_pred = mini_kmean.predict(X_test)\n",
    "    kmean_dist = mini_kmean.transform(X_test)\n",
    "    \n",
    "    kmean_pred = np.array(kmean_pred).reshape((len(kmean_pred),1))\n",
    "    all_preds.append(kmean_pred)\n",
    "    all_preds.append(kmean_dist)\n",
    "    \n",
    "    return np.concatenate(all_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rg: 31 32 33 34 \n",
      "lasso: 31 32 33 34 \n",
      "rf: 31 32 33 34 \n",
      "lgbm: 31 32 33 34 \n"
     ]
    }
   ],
   "source": [
    "stage2_data = stage2_feature_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(885563, 13)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage2_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rg',\n",
       " 'lasso',\n",
       " 'rf',\n",
       " 'lgbm',\n",
       " 'kmean_dist_label',\n",
       " 'kmean_dist_0',\n",
       " 'kmean_dist_1',\n",
       " 'kmean_dist_2',\n",
       " 'kmean_dist_3',\n",
       " 'kmean_dist_4',\n",
       " 'kmean_dist_5',\n",
       " 'kmean_dist_6',\n",
       " 'kmean_dist_7']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['rg', 'lasso', 'rf', 'lgbm'] ##'knn']\n",
    "# columns.extend(['knn_dist_'+str(i) for i in range(15)])\n",
    "columns.extend(['kmean_dist_label'])\n",
    "columns.extend(['kmean_dist_'+str(i) for i in range(8)])\n",
    "print(len(columns))\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_df = pd.DataFrame(data=stage2_data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rg</th>\n",
       "      <th>lasso</th>\n",
       "      <th>rf</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>kmean_dist_label</th>\n",
       "      <th>kmean_dist_0</th>\n",
       "      <th>kmean_dist_1</th>\n",
       "      <th>kmean_dist_2</th>\n",
       "      <th>kmean_dist_3</th>\n",
       "      <th>kmean_dist_4</th>\n",
       "      <th>kmean_dist_5</th>\n",
       "      <th>kmean_dist_6</th>\n",
       "      <th>kmean_dist_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.457809</td>\n",
       "      <td>0.518210</td>\n",
       "      <td>1.104751</td>\n",
       "      <td>1.234071</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.857702</td>\n",
       "      <td>8.712146</td>\n",
       "      <td>6.878428</td>\n",
       "      <td>7.236687</td>\n",
       "      <td>160.683082</td>\n",
       "      <td>8.372746</td>\n",
       "      <td>10.067307</td>\n",
       "      <td>32.780615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.990932</td>\n",
       "      <td>0.951176</td>\n",
       "      <td>1.786170</td>\n",
       "      <td>2.333394</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.806890</td>\n",
       "      <td>13.463366</td>\n",
       "      <td>11.329752</td>\n",
       "      <td>7.886795</td>\n",
       "      <td>162.224312</td>\n",
       "      <td>10.882863</td>\n",
       "      <td>14.441723</td>\n",
       "      <td>33.894320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.473575</td>\n",
       "      <td>0.448425</td>\n",
       "      <td>0.640284</td>\n",
       "      <td>0.663168</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.285941</td>\n",
       "      <td>10.126022</td>\n",
       "      <td>8.115691</td>\n",
       "      <td>6.480298</td>\n",
       "      <td>161.687453</td>\n",
       "      <td>7.767159</td>\n",
       "      <td>11.381747</td>\n",
       "      <td>32.819151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.421733</td>\n",
       "      <td>0.397657</td>\n",
       "      <td>0.457851</td>\n",
       "      <td>0.487791</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.528922</td>\n",
       "      <td>8.592862</td>\n",
       "      <td>6.444938</td>\n",
       "      <td>6.349900</td>\n",
       "      <td>161.312547</td>\n",
       "      <td>7.830429</td>\n",
       "      <td>9.995115</td>\n",
       "      <td>32.689497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.406524</td>\n",
       "      <td>0.390211</td>\n",
       "      <td>0.397525</td>\n",
       "      <td>0.391166</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.568154</td>\n",
       "      <td>8.632067</td>\n",
       "      <td>6.464528</td>\n",
       "      <td>6.335744</td>\n",
       "      <td>161.346125</td>\n",
       "      <td>7.866945</td>\n",
       "      <td>10.024922</td>\n",
       "      <td>32.699838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rg     lasso        rf      lgbm  kmean_dist_label  kmean_dist_0  \\\n",
       "0  0.457809  0.518210  1.104751  1.234071               1.0      8.857702   \n",
       "1  0.990932  0.951176  1.786170  2.333394               5.0     12.806890   \n",
       "2  0.473575  0.448425  0.640284  0.663168               5.0      9.285941   \n",
       "3  0.421733  0.397657  0.457851  0.487791               6.0      8.528922   \n",
       "4  0.406524  0.390211  0.397525  0.391166               6.0      8.568154   \n",
       "\n",
       "   kmean_dist_1  kmean_dist_2  kmean_dist_3  kmean_dist_4  kmean_dist_5  \\\n",
       "0      8.712146      6.878428      7.236687    160.683082      8.372746   \n",
       "1     13.463366     11.329752      7.886795    162.224312     10.882863   \n",
       "2     10.126022      8.115691      6.480298    161.687453      7.767159   \n",
       "3      8.592862      6.444938      6.349900    161.312547      7.830429   \n",
       "4      8.632067      6.464528      6.335744    161.346125      7.866945   \n",
       "\n",
       "   kmean_dist_6  kmean_dist_7  \n",
       "0     10.067307     32.780615  \n",
       "1     14.441723     33.894320  \n",
       "2     11.381747     32.819151  \n",
       "3      9.995115     32.689497  \n",
       "4     10.024922     32.699838  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_io = pd.HDFStore('../data/feat/stage2_data.h5')\n",
    "stage2_io['stage2_df'] = stage2_df\n",
    "stage2_io.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load \n",
    "# with pd.HDFStore('../data/feat/stage2_data.h5') as stage2_io:\n",
    "#     print(stage2_io.keys())\n",
    "#     stage2_df = stage2_io['stage2_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/X_text_feats', '/X_text_feats_cv', '/X_text_feats_test', '/X_text_feats_train']\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore('../data/feat/text_feats.h5') as text_io:\n",
    "    print(text_io.keys())\n",
    "    X_text_feats_test = text_io['X_text_feats_test']\n",
    "    X_text_feats_cv = text_io['X_text_feats_cv']\n",
    "    X_text_feats_train = text_io['X_text_feats_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10675632, 4)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_feats_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     6186922\n",
       "False    4488710\n",
       "Name: date_block_num, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.value_counts() ## reduce memory use only num_date_block >= 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10675632, 4)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_feats_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_df = pd.concat([X_text_feats_train[mask],X_text_feats_cv])\n",
    "test_text_df = X_text_feats_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6639294"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_df.shape[0] + test_text_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_text_df['item_name'].map(str) + ' ' + train_text_df['item_category_name'].map(str) + ' ' + train_text_df['shop_name'].map(str)\n",
    "test_texts = test_text_df['item_name'].map(str) + ' ' + test_text_df['item_category_name'].map(str) + ' ' + test_text_df['shop_name'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6639294,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts = pd.Series(np.concatenate([train_texts, test_texts], axis=0))\n",
    "del train_text_df, test_text_df, train_texts, test_texts; gc.collect()\n",
    "all_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_text_feats_train,X_text_feats_cv,X_text_feats_test; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TFIDF - Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(lowercase=False, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6639294, 63999)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_features = tv.fit_transform(all_texts)\n",
    "tv_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=20, random_state=0)\n",
    "tv_svd_features = svd.fit_transform(tv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9893320053815842"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(tv_svd_features)/(1024*1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-IDF(binarize)- Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvb_features = tv_features.astype(bool).astype(float)\n",
    "del tv_features; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6639294, 20)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=20, random_state=0)\n",
    "tvb_svd_features = svd.fit_transform(tvb_features)\n",
    "tvb_svd_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9893320053815842"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(tvb_svd_features)/(1024*1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hasing + Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(ngram_range=(1, 2), lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6639294, 1048576)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hv_features = hv.fit_transform(all_texts).tocsr()\n",
    "hv_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6639294, 20)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=20, random_state=0)\n",
    "hv_svd_features = svd.fit_transform(hv_features)\n",
    "hv_svd_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9893320053815842"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(hv_svd_features)/(1024*1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hasing(binarize) + Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvb_features = hv_features.astype(bool).astype(float)\n",
    "del hv_features; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6639294, 20)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=20, random_state=0)\n",
    "hvb_svd_features = svd.fit_transform(hvb_features)\n",
    "hvb_svd_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9893320053815842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sys.getsizeof(hvb_svd_features)/(1024*1024*1024))\n",
    "del hvb_features; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stack them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6639294, 80)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features = np.concatenate([tv_svd_features, tvb_svd_features, hv_svd_features, hvb_svd_features], axis=1)\n",
    "text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9573277086019516"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(text_features)/(1024*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features_df = pd.DataFrame(data=text_features, columns=['text_f_'+str(i) for i in range(80)])\n",
    "\n",
    "text_io = pd.HDFStore('../data/feat/text_feat_df.h5') \n",
    "text_io['text_feats_df'] = text_features_df\n",
    "text_io.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del all_texts,hv,hv_svd_features,hvb_svd_features, km, text_features;gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate stage2 text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reload data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore('../data/feat/text_feat_df.h5') as text_io:\n",
    "    text_features_df = text_io['text_feats_df'] ## 12-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.concat([X_train, X_cv,X_test], ignore_index=True)\n",
    "merge_dates = np.concatenate([train_dates,cv_dates, test_dates], axis=0) ## 12-34\n",
    "merge_y = np.concatenate([y_train,y_cv, np.zeros((len(test_dates),))], axis=0) ## 12-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6639294,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6639294, 54)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6186922,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dates.shape # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level2_date_block = [27,28,29,30,31,32]\n",
    "level2_date_block = [31,32]\n",
    "stage2_train_dates = merge_dates[merge_dates>=level2_date_block[0]] # 27-34\n",
    "stage2_train_mask = (stage2_train_dates <= level2_date_block[-1]) # 27-32\n",
    "stage2_valid_mask = (stage2_train_dates == 33) # 33\n",
    "stage2_final_train_mask = (stage2_train_dates <= 33) # 27-33\n",
    "stage2_test_mask = (stage2_train_dates==34) # 34\n",
    "stage2_train_y = merge_y[merge_dates>=level2_date_block[0]] # 27-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6639294, 80)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features_df.shape ## text_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_f_0</th>\n",
       "      <th>text_f_1</th>\n",
       "      <th>text_f_2</th>\n",
       "      <th>text_f_3</th>\n",
       "      <th>text_f_4</th>\n",
       "      <th>text_f_5</th>\n",
       "      <th>text_f_6</th>\n",
       "      <th>text_f_7</th>\n",
       "      <th>text_f_8</th>\n",
       "      <th>text_f_9</th>\n",
       "      <th>...</th>\n",
       "      <th>text_f_70</th>\n",
       "      <th>text_f_71</th>\n",
       "      <th>text_f_72</th>\n",
       "      <th>text_f_73</th>\n",
       "      <th>text_f_74</th>\n",
       "      <th>text_f_75</th>\n",
       "      <th>text_f_76</th>\n",
       "      <th>text_f_77</th>\n",
       "      <th>text_f_78</th>\n",
       "      <th>text_f_79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.360039</td>\n",
       "      <td>-0.207268</td>\n",
       "      <td>-0.081856</td>\n",
       "      <td>-0.080682</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>-0.046853</td>\n",
       "      <td>-0.001503</td>\n",
       "      <td>0.013742</td>\n",
       "      <td>-0.113411</td>\n",
       "      <td>-0.055193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029253</td>\n",
       "      <td>0.060605</td>\n",
       "      <td>0.968887</td>\n",
       "      <td>-0.167136</td>\n",
       "      <td>-0.085876</td>\n",
       "      <td>0.056333</td>\n",
       "      <td>-0.021442</td>\n",
       "      <td>-0.020925</td>\n",
       "      <td>-0.042606</td>\n",
       "      <td>-0.013391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.392482</td>\n",
       "      <td>-0.247329</td>\n",
       "      <td>-0.109987</td>\n",
       "      <td>-0.133386</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>-0.041723</td>\n",
       "      <td>-0.012253</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>-0.089892</td>\n",
       "      <td>-0.039338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022853</td>\n",
       "      <td>0.059408</td>\n",
       "      <td>0.960539</td>\n",
       "      <td>-0.167536</td>\n",
       "      <td>-0.082899</td>\n",
       "      <td>0.071592</td>\n",
       "      <td>-0.041229</td>\n",
       "      <td>-0.013534</td>\n",
       "      <td>-0.026975</td>\n",
       "      <td>0.029369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.144574</td>\n",
       "      <td>-0.007663</td>\n",
       "      <td>0.043563</td>\n",
       "      <td>0.202108</td>\n",
       "      <td>-0.025260</td>\n",
       "      <td>-0.085843</td>\n",
       "      <td>-0.006691</td>\n",
       "      <td>0.008598</td>\n",
       "      <td>-0.122515</td>\n",
       "      <td>-0.061938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023262</td>\n",
       "      <td>0.053279</td>\n",
       "      <td>0.973456</td>\n",
       "      <td>-0.181098</td>\n",
       "      <td>-0.072753</td>\n",
       "      <td>0.049451</td>\n",
       "      <td>-0.025229</td>\n",
       "      <td>-0.014346</td>\n",
       "      <td>-0.031661</td>\n",
       "      <td>0.004714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.360039</td>\n",
       "      <td>-0.207268</td>\n",
       "      <td>-0.081856</td>\n",
       "      <td>-0.080682</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>-0.046853</td>\n",
       "      <td>-0.001503</td>\n",
       "      <td>0.013742</td>\n",
       "      <td>-0.113411</td>\n",
       "      <td>-0.055193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029253</td>\n",
       "      <td>0.060605</td>\n",
       "      <td>0.968887</td>\n",
       "      <td>-0.167136</td>\n",
       "      <td>-0.085876</td>\n",
       "      <td>0.056333</td>\n",
       "      <td>-0.021442</td>\n",
       "      <td>-0.020925</td>\n",
       "      <td>-0.042606</td>\n",
       "      <td>-0.013391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.056258</td>\n",
       "      <td>0.022157</td>\n",
       "      <td>0.091721</td>\n",
       "      <td>-0.006717</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>-0.016456</td>\n",
       "      <td>-0.011579</td>\n",
       "      <td>0.010112</td>\n",
       "      <td>-0.068304</td>\n",
       "      <td>-0.032104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434330</td>\n",
       "      <td>0.394472</td>\n",
       "      <td>0.824389</td>\n",
       "      <td>-0.210678</td>\n",
       "      <td>-0.447102</td>\n",
       "      <td>-0.487347</td>\n",
       "      <td>-0.153259</td>\n",
       "      <td>-0.203642</td>\n",
       "      <td>-0.281350</td>\n",
       "      <td>-0.100445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_f_0  text_f_1  text_f_2  text_f_3  text_f_4  text_f_5  text_f_6  \\\n",
       "0  0.360039 -0.207268 -0.081856 -0.080682  0.000944 -0.046853 -0.001503   \n",
       "1  0.392482 -0.247329 -0.109987 -0.133386  0.005399 -0.041723 -0.012253   \n",
       "2  0.144574 -0.007663  0.043563  0.202108 -0.025260 -0.085843 -0.006691   \n",
       "3  0.360039 -0.207268 -0.081856 -0.080682  0.000944 -0.046853 -0.001503   \n",
       "4  0.056258  0.022157  0.091721 -0.006717  0.000795 -0.016456 -0.011579   \n",
       "\n",
       "   text_f_7  text_f_8  text_f_9    ...      text_f_70  text_f_71  text_f_72  \\\n",
       "0  0.013742 -0.113411 -0.055193    ...       0.029253   0.060605   0.968887   \n",
       "1  0.002745 -0.089892 -0.039338    ...       0.022853   0.059408   0.960539   \n",
       "2  0.008598 -0.122515 -0.061938    ...       0.023262   0.053279   0.973456   \n",
       "3  0.013742 -0.113411 -0.055193    ...       0.029253   0.060605   0.968887   \n",
       "4  0.010112 -0.068304 -0.032104    ...       0.434330   0.394472   0.824389   \n",
       "\n",
       "   text_f_73  text_f_74  text_f_75  text_f_76  text_f_77  text_f_78  text_f_79  \n",
       "0  -0.167136  -0.085876   0.056333  -0.021442  -0.020925  -0.042606  -0.013391  \n",
       "1  -0.167536  -0.082899   0.071592  -0.041229  -0.013534  -0.026975   0.029369  \n",
       "2  -0.181098  -0.072753   0.049451  -0.025229  -0.014346  -0.031661   0.004714  \n",
       "3  -0.167136  -0.085876   0.056333  -0.021442  -0.020925  -0.042606  -0.013391  \n",
       "4  -0.210678  -0.447102  -0.487347  -0.153259  -0.203642  -0.281350  -0.100445  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6186922, 54)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6186922, 134)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.reset_index(drop=True,inplace=True)\n",
    "X_train = pd.concat([X_train,text_features_df.iloc[0:X_train.shape[0],:]], axis=1)\n",
    "X_train.reset_index(drop=True,inplace=True) ## \n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train.shape ## 12-32\n",
    "# y_train.shape ## \n",
    "# test_dates.shape[0]\n",
    "cv_dates.shape[0] + test_dates.shape[0] + train_dates.shape[0] == text_features_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0] + test_dates.shape[0]  +text_features_df.iloc[X_train.shape[0]:-test_dates.shape[0]].shape[0] == text_features_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.reset_index(drop=True,inplace=True)\n",
    "\n",
    "val_text_features_df = text_features_df.iloc[X_train.shape[0]:-test_dates.shape[0]]\n",
    "val = pd.concat([X_cv.reset_index(drop=True), val_text_features_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "test_text_features_df = text_features_df.iloc[-test_dates.shape[0]:,:].reset_index(drop=True)\n",
    "test = pd.concat([X_test.reset_index(drop=True),test_text_features_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape[0] + test.shape[0] + X_train.shape[0] == text_features_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyper params search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple hold out method to optimize data\n",
    "- lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dates.reset_index(drop=True,inplace=True) ##reset index\n",
    "y_train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \n",
    "    clf = params.get('clf')\n",
    "    new_params = params.get('params') ## update params\n",
    "\n",
    "    copy_clf = copy.deepcopy(clf)\n",
    "    original_params = copy_clf.get_params()\n",
    "    original_params.update(new_params)\n",
    "    copy_clf.set_params(**original_params) # update copy clf with trying new_params     \n",
    "    \n",
    "    ## simple hold out \n",
    "    ## train_dates --- 12 - 32\n",
    "    \n",
    "    \n",
    "    train_mask = train_dates < level2_date_block[0] # 12-26\n",
    "    ## simple hold out \n",
    "    validation_mask = (train_dates >= level2_date_block[0]) & (train_dates <= level2_date_block[-1]) ## validate on the level2_date_block\n",
    "    \n",
    "    copy_clf.fit(X_train[train_mask].values, y_train[train_mask]) #  \n",
    "    \n",
    "    pred_y = copy_clf.predict(Xtrain[validation_mask].values) ## 27-32\n",
    "    pred_y = np.clip(pred_y, 0., 20.) # clip to (0,20)\n",
    "    rmse = mean_squared_error(y_train[validation_mask], pred_y)**.5\n",
    "    print('rmse: {:.5f} '.format(rmse), end='\\t')\n",
    "    print('params: {}'.format(new_params))\n",
    "    \n",
    "    return {'loss':rmse,'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.84611 \tparams: {'bagging_fraction': 0.8764806768876663, 'boosting_type': 'gbdt', 'feature_fraction': 0.745557918154995, 'gamma': 0.1464456375669855, 'max_depth': 6, 'num_leaves': 12, 'reg_lambda': 0.9718643064786536}\n",
      "rmse: 0.83181 \tparams: {'bagging_fraction': 0.9370654441376777, 'boosting_type': 'gbdt', 'feature_fraction': 0.8711304569491714, 'gamma': 0.3710785669038915, 'max_depth': 14, 'num_leaves': 30, 'reg_lambda': 0.15222800799554792}\n",
      "rmse: 0.82556 \tparams: {'bagging_fraction': 0.813468222831279, 'boosting_type': 'gbdt', 'feature_fraction': 0.8794022556258285, 'gamma': 0.3999302137394257, 'max_depth': 10, 'num_leaves': 98, 'reg_lambda': 0.11292226001484129}\n",
      "rmse: 0.85294 \tparams: {'bagging_fraction': 0.89355042600479, 'boosting_type': 'gbdt', 'feature_fraction': 0.5405792599265297, 'gamma': 0.16249552714546894, 'max_depth': 4, 'num_leaves': 60, 'reg_lambda': 0.05422990603624822}\n",
      "rmse: 0.83568 \tparams: {'bagging_fraction': 0.7562445089124552, 'boosting_type': 'gbdt', 'feature_fraction': 0.31753135592593507, 'gamma': 0.3439663244614013, 'max_depth': 6, 'num_leaves': 42, 'reg_lambda': 0.9241089175427034}\n",
      "rmse: 0.82775 \tparams: {'bagging_fraction': 0.8045396971992194, 'boosting_type': 'gbdt', 'feature_fraction': 0.4142199582566836, 'gamma': 0.1520052405060338, 'max_depth': 14, 'num_leaves': 56, 'reg_lambda': 0.10091544739332303}\n",
      "rmse: 0.82598 \tparams: {'bagging_fraction': 0.9393021362068352, 'boosting_type': 'gbdt', 'feature_fraction': 0.8507950440359442, 'gamma': 0.2732174210055819, 'max_depth': 8, 'num_leaves': 64, 'reg_lambda': 0.29832232549934257}\n",
      "rmse: 0.82321 \tparams: {'bagging_fraction': 0.7500480439201039, 'boosting_type': 'gbdt', 'feature_fraction': 0.48975069618563466, 'gamma': 0.20060103590950945, 'max_depth': 11, 'num_leaves': 54, 'reg_lambda': 0.466404536169182}\n",
      "rmse: 0.82980 \tparams: {'bagging_fraction': 0.990696855791075, 'boosting_type': 'gbdt', 'feature_fraction': 0.3152421902628413, 'gamma': 0.4117947815132885, 'max_depth': 7, 'num_leaves': 52, 'reg_lambda': 0.3296225071666642}\n",
      "rmse: 0.84053 \tparams: {'bagging_fraction': 0.7112290068259923, 'boosting_type': 'gbdt', 'feature_fraction': 0.5243148330196518, 'gamma': 0.1555186261890245, 'max_depth': 5, 'num_leaves': 32, 'reg_lambda': 0.5847010520233584}\n",
      "rmse: 0.83432 \tparams: {'bagging_fraction': 0.8711103705010387, 'boosting_type': 'gbdt', 'feature_fraction': 0.4167056691079873, 'gamma': 0.3779792903656519, 'max_depth': 6, 'num_leaves': 76, 'reg_lambda': 0.6881470966432084}\n",
      "rmse: 0.83891 \tparams: {'bagging_fraction': 0.9663728623848267, 'boosting_type': 'gbdt', 'feature_fraction': 0.4032224758726725, 'gamma': 0.25978279757496703, 'max_depth': 5, 'num_leaves': 34, 'reg_lambda': 0.5621855514826312}\n",
      "rmse: 0.82850 \tparams: {'bagging_fraction': 0.9848756556656225, 'boosting_type': 'gbdt', 'feature_fraction': 0.990622355595774, 'gamma': 0.13316129060773663, 'max_depth': 10, 'num_leaves': 38, 'reg_lambda': 0.697557009481483}\n",
      "rmse: 0.83392 \tparams: {'bagging_fraction': 0.8322025207719783, 'boosting_type': 'gbdt', 'feature_fraction': 0.673975407688437, 'gamma': 0.38085660702588287, 'max_depth': 12, 'num_leaves': 22, 'reg_lambda': 0.02726952408691319}\n",
      "rmse: 0.83598 \tparams: {'bagging_fraction': 0.8779070135594766, 'boosting_type': 'gbdt', 'feature_fraction': 0.6283525554631079, 'gamma': 0.4492150037291288, 'max_depth': 8, 'num_leaves': 22, 'reg_lambda': 0.27062093474222404}\n",
      "rmse: 0.83766 \tparams: {'bagging_fraction': 0.745286014549493, 'boosting_type': 'gbdt', 'feature_fraction': 0.3800810913716343, 'gamma': 0.40171351917091613, 'max_depth': 16, 'num_leaves': 20, 'reg_lambda': 0.9603975334855057}\n",
      "rmse: 0.85157 \tparams: {'bagging_fraction': 0.9876505819382022, 'boosting_type': 'gbdt', 'feature_fraction': 0.4329338626356537, 'gamma': 0.27009244494092055, 'max_depth': 4, 'num_leaves': 122, 'reg_lambda': 0.7996115754970091}\n",
      "rmse: 0.82701 \tparams: {'bagging_fraction': 0.9171485448731338, 'boosting_type': 'gbdt', 'feature_fraction': 0.8595988175089257, 'gamma': 0.11036069111198415, 'max_depth': 13, 'num_leaves': 128, 'reg_lambda': 0.8775892811664319}\n",
      "rmse: 0.82632 \tparams: {'bagging_fraction': 0.7401530672377193, 'boosting_type': 'gbdt', 'feature_fraction': 0.4130414714493612, 'gamma': 0.19568805309517084, 'max_depth': 16, 'num_leaves': 92, 'reg_lambda': 0.2711745256475676}\n",
      "rmse: 0.82973 \tparams: {'bagging_fraction': 0.958670919138092, 'boosting_type': 'gbdt', 'feature_fraction': 0.47943140532878864, 'gamma': 0.2685787191457832, 'max_depth': 15, 'num_leaves': 40, 'reg_lambda': 0.687592303149239}\n",
      "rmse: 0.82585 \tparams: {'bagging_fraction': 0.7896994425160806, 'boosting_type': 'gbdt', 'feature_fraction': 0.98332089204296, 'gamma': 0.49600146100062714, 'max_depth': 10, 'num_leaves': 98, 'reg_lambda': 0.18086860094538504}\n",
      "rmse: 0.82452 \tparams: {'bagging_fraction': 0.7013136411662739, 'boosting_type': 'gbdt', 'feature_fraction': 0.6072570276178857, 'gamma': 0.21723440181210715, 'max_depth': 11, 'num_leaves': 54, 'reg_lambda': 0.4343536699934936}\n",
      "rmse: 0.83086 \tparams: {'bagging_fraction': 0.7002102040991617, 'boosting_type': 'gbdt', 'feature_fraction': 0.6269167039460164, 'gamma': 0.21840005263041756, 'max_depth': 11, 'num_leaves': 54, 'reg_lambda': 0.4297829727792763}\n",
      "rmse: 0.82732 \tparams: {'bagging_fraction': 0.7700321068993944, 'boosting_type': 'gbdt', 'feature_fraction': 0.5554327123037893, 'gamma': 0.20801239741590197, 'max_depth': 11, 'num_leaves': 114, 'reg_lambda': 0.4399167982946686}\n",
      "rmse: 0.82830 \tparams: {'bagging_fraction': 0.7183214957239468, 'boosting_type': 'gbdt', 'feature_fraction': 0.7639275808815857, 'gamma': 0.2343455853135831, 'max_depth': 11, 'num_leaves': 54, 'reg_lambda': 0.4036823737248517}\n",
      "rmse: 0.82172 \tparams: {'bagging_fraction': 0.7257895345351948, 'boosting_type': 'gbdt', 'feature_fraction': 0.6932767211963933, 'gamma': 0.3004315340493337, 'max_depth': 9, 'num_leaves': 80, 'reg_lambda': 0.519291939932697}\n",
      "rmse: 0.82480 \tparams: {'bagging_fraction': 0.7315210139440134, 'boosting_type': 'gbdt', 'feature_fraction': 0.7008502509118817, 'gamma': 0.3234061222335251, 'max_depth': 9, 'num_leaves': 80, 'reg_lambda': 0.5451340419462}\n",
      "rmse: 0.83036 \tparams: {'bagging_fraction': 0.7752293464796641, 'boosting_type': 'gbdt', 'feature_fraction': 0.7381185489623091, 'gamma': 0.299802760170377, 'max_depth': 9, 'num_leaves': 46, 'reg_lambda': 0.7783317359993828}\n",
      "rmse: 0.82213 \tparams: {'bagging_fraction': 0.8409718669077595, 'boosting_type': 'gbdt', 'feature_fraction': 0.8113448908267493, 'gamma': 0.18285193134826605, 'max_depth': 9, 'num_leaves': 120, 'reg_lambda': 0.5165073429108892}\n",
      "rmse: 0.82243 \tparams: {'bagging_fraction': 0.8498612863195973, 'boosting_type': 'gbdt', 'feature_fraction': 0.8000137745493439, 'gamma': 0.3045900753123663, 'max_depth': 9, 'num_leaves': 108, 'reg_lambda': 0.616823555666372}\n"
     ]
    }
   ],
   "source": [
    "lgb_clf = lgb.LGBMRegressor(random_state=0, n_jobs=8)\n",
    "trials = Trials()\n",
    "\n",
    "lgb_params = {\n",
    "    'clf' : lgb_clf,\n",
    "    'params': {\n",
    "#         'boosting_type': hp.choice('boosting_type',['gbdt', 'dart']), ## gbdt \n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves'   : hp.choice('num_leaves', np.arange(8,129,2,dtype=int)),\n",
    "        'max_depth' : hp.choice(\"max_depth\", np.arange(4, 17, dtype=int)),    \n",
    "        'feature_fraction': hp.uniform('feature_fraction', 0.3, 1.0),\n",
    "        'bagging_fraction': hp.uniform ('bagging_fraction', 0.7, 1),\n",
    "        'reg_lambda': hp.uniform('reg_lambda',0,1),\n",
    "        'gamma' : hp.uniform('gamma', 0.1,0.5)\n",
    "    }\n",
    "    \n",
    "}\n",
    "best =fmin(fn = objective,\n",
    "           space = lgb_params,\n",
    "           algo = tpe.suggest,\n",
    "           trials = trials,\n",
    "           max_evals = 30\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> rmse: `0.82172`\n",
    "\n",
    "- params: \n",
    "\n",
    "{'bagging_fraction': 0.7257895345351948, 'boosting_type': 'gbdt', 'feature_fraction': 0.6932767211963933, 'gamma': 0.3004315340493337, 'max_depth': 9, 'num_leaves': 80, 'reg_lambda': 0.519291939932697}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ridge regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials = Trials()\n",
    "\n",
    "# rg_clf = Pipeline([\n",
    "#     ('standardscaler', StandardScaler(copy=False, with_mean=True, with_std=True)),\n",
    "#     ('rg', Ridge(fit_intercept=True, normalize=True, max_iter=2000, random_state=0))\n",
    "# ])\n",
    "\n",
    "# rg_params = {\n",
    "#     'clf' : rg_clf,\n",
    "#     'params':{\n",
    "#         'rg__alpha':  hp.uniform('rg__alpha', 0., 1.0)\n",
    "#     }\n",
    "    \n",
    "# }\n",
    "# best = fmin(fn = objective,\n",
    "#             space = rg_params,\n",
    "#             algo = tpe.suggest,\n",
    "#             trials = trials,\n",
    "#             max_evals = 10\n",
    "#            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> rmse: 0.961002\n",
    "    - alpha: `0.234133853`\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate stage2 features with TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6639294, 54)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6639294, 80)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.concat([merge,text_features_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4034"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.7257895345351948, boosting_type='gbdt',\n",
       "       class_weight=None, colsample_bytree=1.0,\n",
       "       feature_fraction=0.6932767211963933, gamma=0.3004315340493337,\n",
       "       learning_rate=0.1, max_depth=9, metric='rmse', min_child_samples=20,\n",
       "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
       "       n_jobs=4, num_leaves=80, objective=None, random_state=0,\n",
       "       reg_alpha=0.0, reg_lambda=0.519291939932697, silent=True,\n",
       "       subsample=1.0, subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = lgb.LGBMRegressor(boosting_type='gbdt', \n",
    "                            max_depth=9,\n",
    "                            num_leaves=80,\n",
    "                            bagging_fraction=0.7257895345351948,\n",
    "                            feature_fraction=0.6932767211963933,\n",
    "                            reg_lambda = 0.519291939932697,\n",
    "                            gamma = 0.3004315340493337,\n",
    "                            metric='rmse',\n",
    "                            n_jobs=4,\n",
    "                            random_state=0)\n",
    "\n",
    "model.fit(merge[merge_dates < level2_date_block[0]].values, merge_y[merge_dates < level2_date_block[0]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_data_text = model.predict(merge[merge_dates >= level2_date_block[0]].values)\n",
    "stage2_data_text = np.clip(stage2_data_text, 0., 20.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(885563,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage2_data_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(885563, 134)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge[merge.date_block_num > 30].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save to npy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"temp = np.load('../data/feat/stage2_data_text.npy')\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save('../data/feat/stage2_data_text.npy',stage2_data_text)\n",
    "\n",
    "'''temp = np.load('../data/feat/stage2_data_text.npy')'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
